# Useful starting lines
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

%load_ext autoreload
%autoreload 2
from test_utils import test
import costs





from helpers import load_data

# load dataset
x, y = load_data()


def build_k_indices(y, k_fold, seed):
    """build k indices for k-fold.

    Args:
        y:      shape=(N,)
        k_fold: K in K-fold, i.e. the fold num
        seed:   the random seed

    Returns:
        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold

    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)
    array([[3, 2],
           [0, 1]])
    """
    num_row = y.shape[0]
    interval = int(num_row / k_fold)
    np.random.seed(seed)
    indices = np.random.permutation(num_row)
    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]
    return np.array(k_indices)


test(build_k_indices)





from costs import compute_mse
from ridge_regression import ridge_regression
from build_polynomial import build_poly


test(build_poly)
test(ridge_regression)


def cross_validation(y, x, k_indices, k, lambda_, degree):
    """return the loss of ridge regression for a fold corresponding to k_indices

    Args:
        y:          shape=(N,)
        x:          shape=(N,)
        k_indices:  2D array returned by build_k_indices()
        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)
        lambda_:    scalar, cf. ridge_regression()
        degree:     scalar, cf. build_poly()

    Returns:
        train and test root mean square errors rmse = sqrt(2 mse)

    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)
    (0.019866645527597114, 0.33555914361295175)
    """

    # ***************************************************
    # INSERT YOUR CODE HERE
    # get k'th subgroup in test, others in train: TODO
    # ***************************************************

    if np.ndim(x) >1:
        raise Exception("x should be a unidimensional array")
        
    seed = 12

    
    N = len(x)
    num_fold = len(k_indices)
    num_el = int(np.floor(N/num_fold))
    #compute number of element in training and testing set
    N_tr = N-num_el
    N_te = num_el

    #create the partition of x and y with k_fold different sets with respect to the indices
    part_x = x[k_indices]
    part_y = y[k_indices]

    #isolate the test set of the training data
    y_te = part_y[k]

    x_te = part_x[k]

    #discard the test set, create the training set
    if k < len(part_x):
        
        x_tr = np.concatenate((part_x[:k], part_x[k+1:]))
        y_tr = np.concatenate((part_y[:k], part_y[k+1:]))
    else:
        x_tr = part_x[:k]
        y_tr = part_y[:k]

   
    
    # ***************************************************
    # INSERT YOUR CODE HERE
    # form data with polynomial degree: TODO
    # ***************************************************
    x_poly_tr = build_poly(x_tr.flatten(), degree)
    x_poly_te = build_poly(x_te.flatten(), degree)
    # ***************************************************
    # INSERT YOUR CODE HERE
    # ridge regression: TODO
    # ***************************************************
    y_tr = y_tr.flatten()
    y_te = y_te.flatten()
    
    W = ridge_regression(y_tr, x_poly_tr, lambda_)
    # ***************************************************
    # INSERT YOUR CODE HERE
    # calculate the loss for train and test data: TODO
    # ***************************************************
    
    loss_tr = np.sqrt(2*costs.compute_mse(y_tr, x_poly_tr, W))
    loss_te = np.sqrt(2*costs.compute_mse(y_te, x_poly_te, W))
    
    return loss_tr, loss_te


# can lead to a numerical error if you use an older version than Python 3.9
test(cross_validation)


from plots import cross_validation_visualization


def cross_validation_demo(degree, k_fold, lambdas):
    """cross validation over regularisation parameter lambda.

    Args:
        degree: integer, degree of the polynomial expansion
        k_fold: integer, the number of folds
        lambdas: shape = (p, ) where p is the number of values of lambda to test
    Returns:
        best_lambda : scalar, value of the best lambda
        best_rmse : scalar, the associated root mean squared error for the best lambda
    """

    seed = 12
    degree = degree
    k_fold = k_fold
    lambdas = lambdas
    # split data in k fold
    k_indices = build_k_indices(y, k_fold, seed)
    # define lists to store the loss of training data and test data
    rmse_tr = []
    rmse_te = []
    # ***************************************************
    # INSERT YOUR CODE HERE
    # cross validation over lambdas: TODO
    # ***************************************************
    best_lambda =  np.inf
    best_rmse = np.inf
    error_tr = []
    error_te = []
    
    avg_error_tr = 0
    avg_error_te = 0
    
    for lambda_ in lambdas:
        
        avg_error_tr = 0
        avg_error_te = 0
        
        for k in range(k_fold):
            error_tr, error_te = cross_validation(y, x, k_indices, k, lambda_, degree)
            
            avg_error_tr += 1/k_fold * error_tr
            avg_error_te += 1/k_fold * error_te
        
        rmse_tr.append(avg_error_tr)
        rmse_te.append(avg_error_te)
        
        if avg_error_te < best_rmse:
                
            best_rmse = avg_error_te
            best_lambda = lambda_
                
        
        
    cross_validation_visualization(lambdas, rmse_tr, rmse_te)
    print(
        "For polynomial expansion up to degree %.f, the choice of lambda which leads to the best test rmse is %.5f with a test rmse of %.3f"
        % (degree, best_lambda, best_rmse)
    )
    return best_lambda, best_rmse


best_lambda, best_rmse = cross_validation_demo(7, 4, np.logspace(-4, 0, 30))








best_lambda, best_rmse = cross_validation_demo(10, 4, np.logspace(-10, -2, 30))





def best_degree_selection(degrees, k_fold, lambdas, seed=1):
    """cross validation over regularisation parameter lambda and degree.

    Args:
        degrees: shape = (d,), where d is the number of degrees to test
        k_fold: integer, the number of folds
        lambdas: shape = (p, ) where p is the number of values of lambda to test
    Returns:
        best_degree : integer, value of the best degree
        best_lambda : scalar, value of the best lambda
        best_rmse : value of the rmse for the couple (best_degree, best_lambda)

    >>> best_degree_selection(np.arange(2,11), 4, np.logspace(-4, 0, 30))
    (7, 0.004520353656360241, 0.28957280566456634)
    """

    # split data in k fold
    k_indices = build_k_indices(y, k_fold, seed)

    # ***************************************************
    # INSERT YOUR CODE HERE
    # cross validation over degrees and lambdas: TODO
    # ***************************************************

    best_degree = 0
    best_lambda = 0
    best_rmse = np.inf
    avg_rmse_tr = 0
    avg_rmse_te = 0
    
    for degree in degrees:
        for lambda_ in lambdas:
            avg_rmse_tr = 0
            avg_rmse_te = 0
            for k in range(k_fold):
                rmse_tr, rmse_te = cross_validation(y, x, k_indices, k, lambda_, degree)
                avg_rmse_tr += 1/k_fold * rmse_tr
                avg_rmse_te += 1/k_fold * rmse_te
                
            if avg_rmse_te < best_rmse:
                
                best_degree = degree
                best_lambda = lambda_
                best_rmse = avg_rmse_te

    return best_degree, best_lambda, best_rmse


# can lead to a numerical error if you use an older version than Python 3.9
test(best_degree_selection)

best_degree, best_lambda, best_rmse = best_degree_selection(
    np.arange(2, 11), 4, np.logspace(-4, 0, 30)
)
print(
    "The best rmse of %.3f is obtained for a degree of %.f and a lambda of %.5f."
    % (best_rmse, best_degree, best_lambda)
)





# true function we want to learn
def f_star(x):
    return x**3 - x**2 + 0.5


# plotting function for f_star
def plot_fstar(ax):
    xvals = np.arange(-1, 1, 0.01)
    ax.plot(xvals, f_star(xvals), linestyle="--", color="k", label="f_star")
    ax.set_ylim(-2, 2)


# helper plot function
def plot_poly(x, y, weights, degree, ax, alpha=0.3):
    xvals = np.arange(-1, 1, 0.01)
    tx = build_poly(xvals, degree)
    f = tx.dot(weights)
    ax.plot(xvals, f, color="orange", alpha=alpha)
    ax.scatter(x, y, color="b", alpha=alpha, s=10)
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_title("Polynomial degree " + str(degree))
    ax.set_ylim(-1, 2)


# helper plot function
def plot_f(weights, degree, ax, label=None):
    xvals = np.arange(-1, 1, 0.01)
    tx = build_poly(xvals, degree)
    f = tx.dot(weights)
    ax.plot(xvals, f, color="black", alpha=1, label=label)
    ax.set_ylim(-1, 2)





from least_squares import least_squares


def bias_variance_one_seed(sigma, degrees, seed):
    """One run of the optimal fit for 15 random points and different polynomial expansion degrees.

    Args:
        sigma: scalar, noise variance
        degrees: shape = (3,), 3 different degrees to consider
        seed: integer, random see
    Returns:
    """

    # we will generate 15 random datapoints from the [-1, 1] uniform distribuion
    num_data = 15
    np.random.seed(seed)  # set random seed for reproducibility
    xs = np.random.uniform(-1, 1, num_data)
    # the outputs will be f_star(x) + some random gaussian noise of variance sigma**2
    ys = f_star(xs) + sigma * np.random.randn(num_data)

    fig, axs = plt.subplots(1, len(degrees), figsize=(20, 5))
    for index_degree, degree in enumerate(degrees):
        # ***************************************************
        # INSERT YOUR CODE HERE
        # ***************************************************
        x = build_poly(xs, degree) # (N, degree)
        W, mse = least_squares(ys, x) # (degree, 1)
        plot_poly(xs, ys, W, degree, axs[index_degree]) # x in (N, degree) ys in (N,), 
        plot_fstar(axs[index_degree])
        axs[index_degree].legend()
    plt.show()


bias_variance_one_seed(0.1, [1, 3, 6], seed=2)








def bias_variance_demo(sigma, degrees):
    """Illustration of the bias-variance tradeoff.

    Args:
        sigma: scalar, noise variance
        degrees: shape = (3,), 3 different degrees to consider
    Returns:
    """
    # define parameters
    seeds = range(400)  # number of runs
    num_data = 15

    fig, axs = plt.subplots(1, len(degrees), figsize=(20, 5))
    for index_degree, degree in enumerate(degrees):
        # ***************************************************
        # INSERT YOUR CODE HERE
        # ***************************************************
        w_avg = 0
        for seed in seeds:
            
            np.random.seed(seed)  # set random seed for reproducibility
            xs = np.random.uniform(-1, 1, num_data)
            ys = f_star(xs) + sigma * np.random.randn(num_data)

            x = build_poly(xs, degree) # (N, degree)
            W, mse = least_squares(ys, x) # (degree, 1)
            w_avg += 1/400 * W
            plot_poly(xs, ys, W, degree, axs[index_degree]) # x in (N, degree) ys in (N,)
            
        xs = np.random.uniform(-1, 1, num_data)
        ys = f_star(xs) + sigma * np.random.randn(num_data)
        
        plot_poly(xs, ys, w_avg, degree, axs[index_degree])
        axs[index_degree].legend()
        plot_fstar(axs[index_degree])
        

    plt.show()


bias_variance_demo(0.1, [1, 3, 6])






